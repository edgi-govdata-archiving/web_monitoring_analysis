{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "import csv\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import fnmatch\n",
    "\n",
    "# EDGI's web monitoring scripts\n",
    "!git clone https://github.com/edgi-govdata-archiving/web-monitoring-processing &>/dev/null; \n",
    "!pip install -r web-monitoring-processing/requirements.txt &>/dev/null;\n",
    "!python web-monitoring-processing/setup.py develop &>/dev/null;\n",
    "from web_monitoring import internetarchive;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help us count terms\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "all_stopwords = default_stopwords\n",
    "\n",
    "def count(term, visible_text): # this function counts single word terms from the decoded HTML\n",
    "    term = term.lower()  # normalize so as to make result case insensitive\n",
    "    tally = 0\n",
    "    for section in visible_text:\n",
    "        ##bigram here. instead of section.split, bigram the section\n",
    "        for token in section.split():\n",
    "            token = re.sub(r'[^\\w\\s]','',token)#remove punctuation\n",
    "            tally += int(term == token.lower()) # instead of in do ==\n",
    "    #print(term, tally)\n",
    "    return tally\n",
    "\n",
    "def two_count (term, visible_text): # this function counts phrases from the decoded HTML\n",
    "    tally = 0\n",
    "    length = len(term)\n",
    "    for section in visible_text:\n",
    "        tokens = nltk.word_tokenize(section)\n",
    "        tokens = [x.lower() for x in tokens] # standardize to lowercase\n",
    "        tokens = [re.sub(r'[^\\w\\s]','',x) for x in tokens]\n",
    "        grams=nltk.ngrams(tokens,length)\n",
    "        fdist = nltk.FreqDist(grams)\n",
    "        tally += fdist[term[0].lower(), term[1].lower()]\n",
    "    #print(term, tally)    \n",
    "    return tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "terms = ['climate', ['climate', 'change']] # The terms we want to count\n",
    "dates = [2016,1,1, 2016,7,1] # Looking for snapshots between Jan 1 2016 and July 1 2016, working backwards.\n",
    "pages = ['epa.gov', 'epa.gov/climatechange'] # The pages we want to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_urls= {}\n",
    "row_count = len(pages)\n",
    "column_count = len(terms) \n",
    "matrix = numpy.full((row_count,column_count), 999, dtype=numpy.int16) # Default is 999 until counted otherwise\n",
    "print(\"Looking for \"+str(column_count)+\" terms on \"+str(row_count)+\" pages.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go get a snapshot of the page\n",
    "for pos, row in enumerate(pages):\n",
    "      thisPage = row #change for specific CSVs\n",
    "      final_urls[thisPage]=\"\"\n",
    "      try:\n",
    "          with internetarchive.WaybackClient() as client:\n",
    "               dump = client.list_versions(thisPage, from_date=datetime(dates[0], dates[1],dates[2]), to_date=datetime(dates[3], dates[4], dates[5])) # list_versions calls the CDX API from internetarchive.py from the webmonitoring repo\n",
    "               versions = reversed(list(dump))\n",
    "               for version in versions: # For each version in all the snapshots\n",
    "                   if version.status_code == '200' or version.status_code == '-': # If the IA snapshot was viable...\n",
    "                      url=version.raw_url\n",
    "                      contents = requests.get(url, timeout=120).content.decode() # Decode the url's HTML # Handle the request so that it doesn't hang\n",
    "                      contents = BeautifulSoup(contents, 'lxml')\n",
    "                      body=contents.find('body')\n",
    "                      d=[s.extract() for s in body('footer')]\n",
    "                      d=[s.extract() for s in body('header')]\n",
    "                      d=[s.extract() for s in body('nav')]\n",
    "                      d=[s.extract() for s in body('script')]\n",
    "                      d=[s.extract() for s in body('style')]\n",
    "                      del d\n",
    "                      body=[text for text in body.stripped_strings]\n",
    "                      for p, t in enumerate(terms):\n",
    "                          if type(t) is list:\n",
    "                              page_sum = two_count(t, body)\n",
    "                          else:\n",
    "                              page_sum = count(t, body)\n",
    "                          matrix[pos][p]=page_sum\n",
    "                      final_urls[thisPage]=url\n",
    "                      print(\"Done!\")\n",
    "                      break\n",
    "                   else:\n",
    "                      pass\n",
    "      except:\n",
    "          print(\"No snapshot or can't decode\", thisPage)\n",
    "          final_urls[thisPage]=\"\"\n",
    "          matrix[pos]=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report results\n",
    "for pos,term in enumerate(terms):\n",
    "    for p, page in enumerate(pages):\n",
    "        t = \"\"\n",
    "        if type(term) == list:\n",
    "            for part in term:\n",
    "                t += part + \" \"\n",
    "            t = t[:-1]\n",
    "        else:\n",
    "            t = term\n",
    "        print(\"'\"+ t +\"': \"+ str(matrix[p][pos]) +\" on \" + pages[p] + \" (\"+final_urls[pages[p]]+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
